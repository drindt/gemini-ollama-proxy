# -----------------------------------------------------------------------------
# Gemini Proxy Environment Configuration
#
# Copy this file to .env and fill in your details.
# The .env file is ignored by git, so your secrets will be safe.
# -----------------------------------------------------------------------------

# --- Google Gemini API Settings ---

# API key for Google Gen AI.
# A free key is available at: https://aistudio.google.com/apikey
# IMPORTANT: The proxy will not work without a valid API key.
GENAI_API_KEY="Get your own API key from https://aistudio.google.com/apikey"

# --- Network Settings ---

# Hostname or IP address on which the service should run.
# Use '0.0.0.0' to make the service accessible from outside the container/machine.
# Use '127.0.0.1' or 'localhost' to restrict access to the local machine.
# Defaults to '0.0.0.0'.
HOST="0.0.0.0"

# The network port the proxy will listen on.
# The default is 11434, which is the same port used by the real Ollama service.
# This allows the proxy to serve as a transparent, drop-in replacement for a
# local Ollama instance without needing to reconfigure your client applications
# (e.g., Open WebUI, programming scripts).
PORT=11434

# --- Logging Configuration ---

# LOG_LEVEL determines the detail level of logging and debug output.
# - debug:   Detailed logs for troubleshooting and development.
# - info:    Standard logging with important processing steps (recommended for production).
# - warning: Only warnings and errors are logged.
# - error:   Only critical errors are logged.
LOG_LEVEL=warning
